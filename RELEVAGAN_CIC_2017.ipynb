{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU553Jumx05t"
   },
   "source": [
    "# RELEVAGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRWaG6qPWj1T"
   },
   "source": [
    "<a id=\"CGAN\"><h1>Import Header</h1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4298,
     "status": "ok",
     "timestamp": 1646286467366,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "adrd84AgicLV",
    "outputId": "5efbe38a-5301-44fd-d78b-ccb5e21cd524"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/PhD/Development/code/RELEVAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1391,
     "status": "ok",
     "timestamp": 1646286468746,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "mTwKRaEVjJ8c",
    "outputId": "919b98ff-2212-4330-ea6a-1b32ba525777"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install gym\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2\n",
    "\n",
    "# !pip install keras-rl2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1646286468748,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "ljvKEMMhp8lR"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/PhD/Development/code/RELEVAGAN\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4741,
     "status": "ok",
     "timestamp": 1646286473469,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "ZlXu5pxhWj1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nu/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import header\n",
    "\n",
    "importlib.reload(header) # For reloading after making changes\n",
    "from header import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "aQkaphklFIYW"
   },
   "source": [
    "<a id=\"CGAN\"><h1>Select GAN and Dataset and Flags</h1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1646286473470,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "kPfnVUMHWj1h"
   },
   "outputs": [],
   "source": [
    "GAN_type = 'RELEVAGAN_CC'\n",
    "# GAN_type = 'ACGAN_CV'\n",
    "# GAN_type = 'EVAGAN_CV'\n",
    "\n",
    "\n",
    "# DATA_SET = 'ISCX-2014'\n",
    "DATA_SET = 'CIC-2017'\n",
    "#DATA_SET = 'CIC-2018'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zsYaqGjWj1j"
   },
   "source": [
    "<a id=\"GPU Settings\"><h2>Set Flags</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1646286473471,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "S1cf7rM3Wj1k"
   },
   "outputs": [],
   "source": [
    "begin_from_start = 0\n",
    "take_chunk = 0\n",
    "required_epochs = 150\n",
    "\n",
    "DISPLAY_FEATURES = 0\n",
    "EVALUATION_PARAMETER = 'Accuracy'\n",
    "SAVE_ONLY_BOT_DATA = 0\n",
    "USE_KMEANS_FOR_CLASSIFICATION = 1\n",
    "\n",
    "BALANCE_THE_DATASET = 1\n",
    "\n",
    "labels =[]\n",
    "\n",
    "USE_ONLY_TRAIN_SET = 1\n",
    "\n",
    "USE_ALL_CLASSIFIERS = 0\n",
    "\n",
    "ACCU_EVAL_TEST = 0\n",
    "RCL_EVAL_TEST = 0\n",
    "\n",
    "VISUAL_TEST_OVERLAPPING = 1\n",
    "\n",
    "CSV_ONE_BOT = 0\n",
    "\n",
    "VIEW_ALL_BOTS = 0\n",
    "\n",
    "CTU_NERIS = 0\n",
    "\n",
    "SINGLE_WEIGHT_CLASSIFIER_TEST_C2ST = 0\n",
    "SINGLE_WEIGHT_CLASSIFIER_TEST_PROPOSED_METHODOLOGY = 0\n",
    "\n",
    "C2ST_BLACK_BOX_TEST = 0\n",
    "BOTSHOT_BLACK_BOX_TEST = 0\n",
    "\n",
    "C2ST_BLACK_BOX_TEST_AFTER_GAN_TRAINING = 0\n",
    "BOTSHOT_BLACK_BOX_TEST_AFTER_GAN_TRAINING = 0\n",
    "\n",
    "GENERATE_OTHERS_DATA = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbFYC_WQWj1n"
   },
   "source": [
    "<a id=\"CGAN\"><h1>Set Paths</h1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1646286473472,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "8Al8rt2iWj1p"
   },
   "outputs": [],
   "source": [
    "MAIN_CODE_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1646286473472,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "WrizUpKsYxel"
   },
   "outputs": [],
   "source": [
    "DATA_SET_PATH = MAIN_CODE_PATH + '/Dataset/' +  DATA_SET + '/'\n",
    "CACHE_PATH = MAIN_CODE_PATH + '/cache/' + GAN_type + '/'\n",
    "FIGS_PATH = MAIN_CODE_PATH  + '/figs/' + GAN_type + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1646286473473,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "yT1YS8U9Wj1t",
    "outputId": "e4079158-267c-4b8c-f6c5-241c2570f61b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nu/Insync/rhr407@gmail.com/Google Drive/PhD/Development/code/RELEVAGAN_DQN_Agent/Dataset/CIC-2017/\n",
      "/home/nu/Insync/rhr407@gmail.com/Google Drive/PhD/Development/code/RELEVAGAN_DQN_Agent/cache/RELEVAGAN_CC/\n",
      "/home/nu/Insync/rhr407@gmail.com/Google Drive/PhD/Development/code/RELEVAGAN_DQN_Agent/figs/RELEVAGAN_CC/\n"
     ]
    }
   ],
   "source": [
    "print(DATA_SET_PATH)\n",
    "print(CACHE_PATH)\n",
    "print(FIGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwGuWbiSWj1y"
   },
   "source": [
    "<a id=\"GPU Settings\"><h2>Check Available GPUs</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1646286473474,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "jwQXSblnWj10",
    "outputId": "6dc71bbf-2f29-444d-83ae-66148060c55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzSPR7q5Wj11"
   },
   "source": [
    "<a id=\"GPU Settings\"><h2>Import Dataset</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1646286473475,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "w0MQAWmiWj12",
    "outputId": "82533b26-0223-4159-9f62-b488faf6f8c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nu/Insync/rhr407@gmail.com/Google Drive/PhD/Development/code/RELEVAGAN_DQN_Agent/Dataset/CIC-2017\n",
      " CIC_Friday_bot.csv  'CIC_Friday_bot.csv_(Preprocessed).csv'\r\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_SET_PATH\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5361,
     "status": "ok",
     "timestamp": 1646286478822,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "NIJpUfZdWj15",
    "outputId": "f289caa0-4318-4fca-f55a-3a16e3bd78e8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Imported: CIC-2017\n",
      "Training set: (72330, 68)\n"
     ]
    }
   ],
   "source": [
    "if begin_from_start:        \n",
    "\n",
    "    if DATA_SET == 'ISCX-2014':\n",
    "        training_data = prepare_ISCX_2014_data(PATH = DATA_SET_PATH, INPUT_FILE_NAME = r'ISCX_Botnet-Training.pcap_Flow.csv')       \n",
    "    \n",
    "    elif DATA_SET == 'CIC-2017':\n",
    "        training_data = prepare_cic_2017_data(PATH = DATA_SET_PATH, INPUT_FILE_NAME = r'CIC_Friday_bot.csv')\n",
    "                \n",
    "    elif DATA_SET == 'CIC-2018':\n",
    "        training_data = prepare_cic_2018_data(PATH = DATA_SET_PATH, INPUT_FILE_NAME = r'Friday-02-03-2018_TrafficForML_CICFlowMeter.csv')\n",
    "                \n",
    "    elif DATA_SET == 'UNSW_BotIoT':\n",
    "        training_data = prepare_UNSW_IoT(PATH = DATA_SET_PATH, INPUT_FILE_NAME = r'UNSW_2018_IoT_Botnet_Final_10_best_Training.csv')\n",
    "                \n",
    "    elif DATA_SET == 'Darknet':\n",
    "        training_data = prepare_DARKNET_2020_data(PATH = DATA_SET_PATH, INPUT_FILE_NAME = r'Darknet.csv')\n",
    "        \n",
    "\n",
    "    print('Dataset preprocessed: ' + DATA_SET)\n",
    "    \n",
    "else:\n",
    "\n",
    "    if DATA_SET == 'ISCX-2014':\n",
    "        INPUT_TRAINING_FILE_NAME = r'ISCX_Botnet-Training.pcap_Flow.csv_VIRUT'        \n",
    "        \n",
    "    elif DATA_SET == 'CIC-2017':\n",
    "        INPUT_TRAINING_FILE_NAME = r'CIC_Friday_bot.csv'  \n",
    "        \n",
    "    elif DATA_SET == 'CIC-2018':\n",
    "        INPUT_TRAINING_FILE_NAME = r'Friday-02-03-2018_TrafficForML_CICFlowMeter.csv'       \n",
    "        \n",
    "    elif DATA_SET == 'BoT-IoT':\n",
    "        INPUT_TRAINING_FILE_NAME = r'UNSW_2018_IoT_Botnet_Final_10_best_Training.csv'       \n",
    "        \n",
    "    elif DATA_SET == 'Drebin':\n",
    "        INPUT_TRAINING_FILE_NAME = r'Drebin_API_Dataset.csv'                \n",
    "        \n",
    "    elif DATA_SET == 'Darknet':\n",
    "        INPUT_TRAINING_FILE_NAME = r'Darknet.csv'        \n",
    "\n",
    "    training_data = pd.read_csv (INPUT_TRAINING_FILE_NAME + '_(Preprocessed).csv', low_memory=False)\n",
    "    training_data= training_data.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "    print('Dataset Imported: ' + DATA_SET)\n",
    "    print('Training set: '+ str(training_data.shape)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1432,
     "status": "ok",
     "timestamp": 1646286480246,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "eg83hsWRWj16",
    "outputId": "b66f50cf-7738-4ab5-8c7b-e1dcaa61ddff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FlowDuration  TotalFwdPackets  TotalBackwardPackets  \\\n",
      "count  72330.000000     72330.000000          72330.000000   \n",
      "mean       0.214622         0.000057              0.000050   \n",
      "std        0.353401         0.000263              0.000286   \n",
      "min        0.000000         0.000000              0.000004   \n",
      "25%        0.000002         0.000000              0.000004   \n",
      "50%        0.009819         0.000024              0.000014   \n",
      "75%        0.291335         0.000063              0.000046   \n",
      "max        1.000000         0.028342              0.037421   \n",
      "\n",
      "       TotalLengthofFwdPackets  TotalLengthofBwdPackets  FwdPacketLengthMax  \\\n",
      "count             72330.000000             7.233000e+04        72330.000000   \n",
      "mean                  0.001067             2.277297e-05            0.015710   \n",
      "std                   0.005919             2.499196e-04            0.034347   \n",
      "min                   0.000000             0.000000e+00            0.000000   \n",
      "25%                   0.000005             1.000000e-08            0.000242   \n",
      "50%                   0.000304             5.900000e-07            0.008018   \n",
      "75%                   0.000780             7.190000e-06            0.018372   \n",
      "max                   0.505946             3.094099e-02            1.000000   \n",
      "\n",
      "       FwdPacketLengthMin  FwdPacketLengthMean  FwdPacketLengthStd  \\\n",
      "count        72330.000000         72330.000000        72330.000000   \n",
      "mean             0.000937             0.012135            0.017968   \n",
      "std              0.012197             0.029926            0.033667   \n",
      "min              0.000000             0.000000            0.000000   \n",
      "25%              0.000000             0.001010            0.000000   \n",
      "50%              0.000000             0.006030            0.010447   \n",
      "75%              0.000000             0.013358            0.023225   \n",
      "max              1.000000             1.000000            1.000000   \n",
      "\n",
      "       BwdPacketLengthMax  ...  min_seg_size_forward    ActiveMean  \\\n",
      "count        72330.000000  ...          72330.000000  72330.000000   \n",
      "mean             0.067371  ...              0.446824      0.001757   \n",
      "std              0.085038  ...              0.105533      0.012216   \n",
      "min              0.000000  ...              0.357143      0.000000   \n",
      "25%              0.000460  ...              0.357143      0.000000   \n",
      "50%              0.016114  ...              0.357143      0.000000   \n",
      "75%              0.112032  ...              0.571429      0.000799   \n",
      "max              0.896255  ...              0.821429      1.000000   \n",
      "\n",
      "          ActiveStd     ActiveMax     ActiveMin      IdleMean       IdleStd  \\\n",
      "count  72330.000000  72330.000000  72330.000000  72330.000000  72330.000000   \n",
      "mean       0.002510      0.003974      0.001133      0.051708      0.004552   \n",
      "std        0.014285      0.018291      0.010881      0.118541      0.037376   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.001679      0.000329      0.081680      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      0.934726   \n",
      "\n",
      "            IdleMax       IdleMin         Label  \n",
      "count  72330.000000  72330.000000  72330.000000  \n",
      "mean       0.053826      0.048643      0.972957  \n",
      "std        0.123402      0.116318      0.162209  \n",
      "min        0.000000      0.000000      0.000000  \n",
      "25%        0.000000      0.000000      1.000000  \n",
      "50%        0.000000      0.000000      1.000000  \n",
      "75%        0.083333      0.066270      1.000000  \n",
      "max        1.000000      1.000000      1.000000  \n",
      "\n",
      "[8 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "training_data = training_data.replace([np.inf, -np.inf], np.nan).dropna(how=\"any\").reset_index(drop=True)\n",
    "print(training_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5phnoDzWj17"
   },
   "source": [
    "<a id=\"GPU Settings\"><h2>Display Features</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1646286480247,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "aePYFbaoWj17",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DISPLAY_FEATURES: \n",
    "    unified_df = training_data.copy()\n",
    "    X_cols = unified_df.columns[:-1]\n",
    "    y_cols = unified_df.columns[-1]\n",
    "\n",
    "\n",
    "\n",
    "    axarr = [[]]*len(X_cols)\n",
    "    columns = 4\n",
    "    rows = int( np.ceil( len(X_cols) / columns ) )\n",
    "    f, fig = plt.subplots( figsize=(columns*2.5, rows*2) )\n",
    "\n",
    "    f.suptitle('Data Distributions by Feature and Label', size=16)\n",
    "\n",
    "    for i, col in enumerate(X_cols[:]):\n",
    "        axarr[i] = plt.subplot2grid( (int(rows), int(columns)), (int(i//columns), int(i%columns)) )\n",
    "\n",
    "\n",
    "        axarr[i].hist( unified_df.loc[ unified_df.Label == 0, col ] , label=['Normal'], color=('#009933'), alpha=0.5,\n",
    "                              bins=np.linspace( np.percentile(unified_df[col],0), np.percentile(unified_df[col],100),50 ),\n",
    "                              density=True )\n",
    "\n",
    "        axarr[i].hist( unified_df.loc[ unified_df.Label == 1, col ] , label=['Real Bot'], color=['#FF0000'], alpha=0.5,\n",
    "                              bins=np.linspace( np.percentile(unified_df[col],0), np.percentile(unified_df[col],100),50 ),\n",
    "                              density=True )\n",
    "\n",
    "        axarr[i].set_xlabel(col, size=12)\n",
    "    #     axarr[i].set_ylim([0,1])\n",
    "        axarr[i].tick_params(axis='both', labelsize=10)\n",
    "        if i == 0: \n",
    "            legend = axarr[i].legend()\n",
    "            legend.get_frame().set_facecolor('white')\n",
    "        if i%4 != 0 : \n",
    "            axarr[i].tick_params(axis='y', left=True, labelleft=True)\n",
    "        else:\n",
    "            axarr[i].set_ylabel('Fraction',size=12)\n",
    "\n",
    "    plt.tight_layout(rect=[0,0,1,0.95]) # xmin, ymin, xmax, ymax\n",
    "    # plt.savefig('plots/Engineered_Data_Distributions.png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# else: \n",
    "#     print('Pair Plotting..')\n",
    "# #     sns.pairplot(training_data, hue=\"Label\")\n",
    "    \n",
    "#     sns.pairplot(training_data, vars=['Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
    "#        'Total Length of Fwd Packet', 'Total Length of Bwd Packet'], hue=\"Label\")\n",
    "    \n",
    "#     sns.pairplot(penguins, hue=\"species\", markers=[\"o\", \"s\", \"D\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD-Hq093Wj18"
   },
   "source": [
    "<a id=\"GPU Settings\"><h2>Select Botnet</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1646286480248,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "jMdckAGdWj19",
    "outputId": "46fe73e9-a31e-4ad2-ad93-1365278c0e6f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal before chunk: (70374, 68)\n",
      "Real Bots before chunk: (1956, 68)\n",
      "Normal: (70374, 68)\n",
      "Real Bots: (1956, 68)\n"
     ]
    }
   ],
   "source": [
    "normal = training_data.loc[ training_data['Label']==1 ].copy()\n",
    "bots = training_data.loc[ training_data['Label']==0 ].copy()\n",
    "\n",
    "print('Normal before chunk: ' + str(normal.shape))    \n",
    "print('Real Bots before chunk: ' + str(bots.shape)) \n",
    "\n",
    "if take_chunk:\n",
    "    bots = bots[0:512]\n",
    "    \n",
    "print('Normal: ' + str(normal.shape))    \n",
    "print('Real Bots: ' + str(bots.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1646286480249,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "WEv9Tz5AWj1-"
   },
   "outputs": [],
   "source": [
    "Train = training_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1646286480250,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "8TrvNDY3Wj1-",
    "outputId": "fe3d432c-995f-469a-e6e6-b26bacf6ccb7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1956\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "bots_count =  pd.DataFrame( [ [np.sum(bots['Label']==i)] for i in np.unique(bots['Label']) ], columns=['count'], index=np.unique(bots['Label']) )\n",
    "\n",
    "label_cols = [ i for i in bots.columns if 'Label' in i ]\n",
    "data_cols = [ i for i in bots.columns if i not in label_cols ]\n",
    "\n",
    "train_no_label = bots[ data_cols ].reset_index(drop=True)\n",
    "\n",
    "print(bots_count['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1646286481004,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "uKyFTGYfWj1_"
   },
   "outputs": [],
   "source": [
    "train_data = bots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_5UjRLxWj2B"
   },
   "source": [
    "<a id=\"Classification\"><h1>Classification</h1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1646286481006,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "hPMsTK_pWj2B",
    "outputId": "69298b35-df4a-4b68-9100-369f528e2fdb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956\n",
      "CPU times: user 1.72 s, sys: 122 ms, total: 1.84 s\n",
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "# if GAN_type == 'CGAN' or GAN_type == 'WCGAN':\n",
    "if USE_KMEANS_FOR_CLASSIFICATION:\n",
    "    algorithms = [ \n",
    "    #     [ 'KMeans', cluster.KMeans, (), {'random_state':0} ],\n",
    "        [ 'KMeans', cluster.KMeans, (), {'n_clusters':1, 'random_state':0} ],\n",
    "    #     [ 'KMeans 3', cluster.KMeans, (), {'n_clusters':3, 'random_state':0} ],\n",
    "    #     [ 'Agglomerative', cluster.AgglomerativeClustering, (), {} ],\n",
    "    #     [ 'Agglomerative', cluster.AgglomerativeClustering, (), {'linkage': 'ward', 'n_clusters': 3} ],\n",
    "    #     [ 'Agg. Ave 3', cluster.AgglomerativeClustering, (), {'linkage': 'average', 'n_clusters': 3} ],\n",
    "    #     [ 'Agg. Complete 3', cluster.AgglomerativeClustering, (), {'linkage': 'complete', 'n_clusters': 3} ],\n",
    "    #     [ 'DBSCAN', cluster.DBSCAN, (), {'eps':0.025} ],\n",
    "    #     [ 'HDBSCAN', hdbscan.HDBSCAN, (), {} ],\n",
    "    #     [ 'HDBSCAN', hdbscan.HDBSCAN, (), {'min_cluster_size':10, 'min_samples':1, } ],\n",
    "    #     [ 'HDBSCAN 2 10', hdbscan.HDBSCAN, (), {'min_cluster_size':2, 'min_samples':10, } ],\n",
    "    #     [ 'HDBSCAN 10 10 ', hdbscan.HDBSCAN, (), {'min_cluster_size':10, 'min_samples':10, } ],\n",
    "    ]\n",
    "\n",
    "    rows = len(algorithms)\n",
    "    columns = 1\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(3, 2),\n",
    "                            constrained_layout=True)\n",
    "\n",
    "    for i, [name, algorithm, args, kwds] in enumerate(algorithms):\n",
    "\n",
    "        labels = algorithm(*args, **kwds).fit_predict(train_no_label)\n",
    "        print(len(labels))\n",
    "        colors = np.clip(labels,-1,9)\n",
    "        colors = [ 'C'+str(i) if i>-1 else 'white' for i in colors ]\n",
    "\n",
    "        plt.subplot(rows,columns,i*columns+1)\n",
    "        plt.scatter(train_no_label[data_cols[0]], train_no_label[data_cols[1]], c=colors)\n",
    "        plt.xlabel(data_cols[0]), plt.ylabel(data_cols[1])\n",
    "        plt.title(name)\n",
    "            \n",
    "\n",
    "#     else:\n",
    "#         labels = train_bots_only['Label'].values.tolist() \n",
    "#         sns.set(style=\"ticks\", color_codes=True) # Remove background and grid\n",
    "\n",
    "#     #     g = sns.scatterplot(data_cols[0],data_cols[1], data=train, hue=labels)\n",
    "\n",
    "#     #     plt.show() \n",
    "\n",
    "\n",
    "#         plt.figure()\n",
    "#         ax = sns.countplot(y=\"Label\", data=train_bots_only) # for Seaborn version 0.7 and more\n",
    "#         for p in ax.patches:\n",
    "#             ax.text(p.get_y() + p.get_width() + 2700 , p.get_y()+p.get_height()-0.1, p.get_width(), ha=\"center\") \n",
    "\n",
    "#         ax.set_ylabel('Botnets')\n",
    "\n",
    "#         plt.savefig('Botnet-Trainset.pdf', dpi=600)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         plt.figure(figsize=(6, 6))\n",
    "#         ax = sns.countplot(y=\"Label\", data=test_bots_only) # for Seaborn version 0.7 and more\n",
    "#         for p in ax.patches:\n",
    "#             ax.text(p.get_y() + p.get_width() + 6000 , p.get_y()+p.get_height()-0.1, p.get_width(), ha=\"center\") \n",
    "\n",
    "#         ax.set_ylabel('Botnets')\n",
    "\n",
    "#         plt.savefig('Botnet-Testset.pdf', dpi=600)\n",
    "#         plt.show()\n",
    "\n",
    "#     #     g = sns.catplot(x=\"class\", hue=\"who\", col=\"survived\", data=titanic, kind=\"count\", height=4, aspect=.7);\n",
    "\n",
    "\n",
    "#     #     sns.pairplot(data=train, vars=[data_cols[0], data_cols[1]], hue='Label')\n",
    "\n",
    "\n",
    "#     # plt.grid(False)\n",
    "#     # plt.show()\n",
    "#     print(train_no_label.describe())\n",
    "    \n",
    "    botnet_w_classes = train_no_label.copy()\n",
    "    botnet_w_classes['Label'] = labels\n",
    "\n",
    "#     print(botnet_w_classes.describe())\n",
    "    train_data = botnet_w_classes\n",
    "    \n",
    "# else:\n",
    "#     train_data = train_no_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1646286481008,
     "user": {
      "displayName": "Rizwan Hamid Randhawa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCoBmDlAGvdCQjk-uJ2xxx1pJC0fjRsK2FcWLNdaY=s64",
      "userId": "15615777248917371178"
     },
     "user_tz": 0
    },
    "id": "wmB4aCmMWj2D",
    "outputId": "bb6c85f1-3ae0-4e88-99bd-0b6f26f1f70d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1951    0\n",
       "1952    0\n",
       "1953    0\n",
       "1954    0\n",
       "1955    0\n",
       "Name: Label, Length: 1956, dtype: int32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q168UjfTWj2E"
   },
   "source": [
    "<a id=\"GPU Settings\"><h2>GAN Training</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: (2048, 68)\n",
      "log_interval : 8\n",
      "Total Batch Iterations: 1200\n",
      "['FlowDuration', 'TotalFwdPackets', 'TotalBackwardPackets', 'TotalLengthofFwdPackets', 'TotalLengthofBwdPackets', 'FwdPacketLengthMax', 'FwdPacketLengthMin', 'FwdPacketLengthMean', 'FwdPacketLengthStd', 'BwdPacketLengthMax', 'BwdPacketLengthMin', 'BwdPacketLengthMean', 'BwdPacketLengthStd', 'FlowBytes/s', 'FlowPackets/s', 'FlowIATMean', 'FlowIATStd', 'FlowIATMax', 'FlowIATMin', 'FwdIATTotal', 'FwdIATMean', 'FwdIATStd', 'FwdIATMax', 'FwdIATMin', 'BwdIATTotal', 'BwdIATMean', 'BwdIATStd', 'BwdIATMax', 'BwdIATMin', 'FwdPSHFlags', 'FwdHeaderLength', 'BwdHeaderLength', 'FwdPackets/s', 'BwdPackets/s', 'MinPacketLength', 'MaxPacketLength', 'PacketLengthMean', 'PacketLengthStd', 'PacketLengthVariance', 'FINFlagCount', 'SYNFlagCount', 'RSTFlagCount', 'PSHFlagCount', 'ACKFlagCount', 'URGFlagCount', 'ECEFlagCount', 'Down/UpRatio', 'AveragePacketSize', 'AvgFwdSegmentSize', 'AvgBwdSegmentSize', 'FwdHeaderLength.1', 'SubflowFwdPackets', 'SubflowFwdBytes', 'SubflowBwdPackets', 'SubflowBwdBytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'ActiveMean', 'ActiveStd', 'ActiveMax', 'ActiveMin', 'IdleMean', 'IdleStd', 'IdleMax', 'IdleMin']\n",
      "CIC-2017_2022-05-10 10:55:21.545351\n",
      "WARNING:tensorflow:From /home/nu/anaconda3/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "['loss']\n",
      "Normal: (70374, 68)\n",
      "Bots: (1956, 68)\n",
      "(1369, 68) (49261, 68)\n",
      "======================================================\n",
      "Starting GAN Training..\n",
      "======================================================\n",
      "['loss']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nu/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-05-10 10:55:23.893266: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 10:55:25.624130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6003 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\n",
      "2022-05-10 10:55:25.625171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6491 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "2022-05-10 10:55:25.625933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 5768 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nu/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 256 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nu/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/home/nu/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/home/nu/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256/256: episode: 1, duration: 8.681s, episode steps: 256, steps per second:  29, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 5.105 [0.000, 11.000],  loss: 0.294885, mae: 0.757516, mean_q: 1.523243\n",
      "done, took 8.683 seconds\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  g_loss: 1.404856\n",
      "Evasions: 255\n",
      "Time left = 54.6 minutes\n",
      "Total Time Taken: 0.4 minutes\n",
      "epoch_number: 1 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.768s, episode steps: 256, steps per second:  68, episode reward: 254.000, mean reward:  0.992 [ 0.000,  1.000], mean action: 5.219 [0.000, 11.000],  loss: 0.135452, mae: 0.852890, mean_q: 1.606744\n",
      "done, took 3.770 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.815s, episode steps: 256, steps per second:  67, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 5.168 [0.000, 11.000],  loss: 0.086105, mae: 0.877927, mean_q: 1.538393\n",
      "done, took 3.816 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.708s, episode steps: 256, steps per second:  69, episode reward: 253.000, mean reward:  0.988 [ 0.000,  1.000], mean action: 5.098 [0.000, 11.000],  loss: 0.074219, mae: 0.868187, mean_q: 1.442646\n",
      "done, took 3.709 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.859s, episode steps: 256, steps per second:  66, episode reward: 253.000, mean reward:  0.988 [ 0.000,  1.000], mean action: 4.945 [0.000, 11.000],  loss: 0.059107, mae: 0.859898, mean_q: 1.375538\n",
      "done, took 3.861 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.789s, episode steps: 256, steps per second:  68, episode reward: 254.000, mean reward:  0.992 [ 0.000,  1.000], mean action: 4.922 [0.000, 11.000],  loss: 0.054570, mae: 0.866186, mean_q: 1.354709\n",
      "done, took 3.791 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.724s, episode steps: 256, steps per second:  69, episode reward: 250.000, mean reward:  0.977 [ 0.000,  1.000], mean action: 4.891 [0.000, 11.000],  loss: 0.051334, mae: 0.864287, mean_q: 1.327632\n",
      "done, took 3.725 seconds\n",
      "  g_loss: 1.1841213\n",
      "Evasions: 1519\n",
      "Time left = 57.0 minutes\n",
      "Total Time Taken: 0.8 minutes\n",
      "epoch_number: 2 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.775s, episode steps: 256, steps per second:  68, episode reward: 250.000, mean reward:  0.977 [ 0.000,  1.000], mean action: 4.910 [0.000, 11.000],  loss: 0.043109, mae: 0.870615, mean_q: 1.314557\n",
      "done, took 3.777 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.725s, episode steps: 256, steps per second:  69, episode reward: 253.000, mean reward:  0.988 [ 0.000,  1.000], mean action: 4.910 [0.000, 11.000],  loss: 0.038899, mae: 0.864068, mean_q: 1.287530\n",
      "done, took 3.726 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.718s, episode steps: 256, steps per second:  69, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.930 [0.000, 11.000],  loss: 0.038146, mae: 0.882966, mean_q: 1.293836\n",
      "done, took 3.719 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.781s, episode steps: 256, steps per second:  68, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.891 [0.000, 11.000],  loss: 0.036960, mae: 0.880860, mean_q: 1.278364\n",
      "done, took 3.782 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.805s, episode steps: 256, steps per second:  67, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.910 [0.000, 11.000],  loss: 0.034559, mae: 0.892393, mean_q: 1.283259\n",
      "done, took 3.807 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.744s, episode steps: 256, steps per second:  68, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.922 [0.000, 11.000],  loss: 0.031642, mae: 0.893468, mean_q: 1.266811\n",
      "done, took 3.745 seconds\n",
      "  g_loss: 0.90167904\n",
      "Evasions: 1523\n",
      "Time left = 57.6 minutes\n",
      "Total Time Taken: 1.2 minutes\n",
      "epoch_number: 3 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.940s, episode steps: 256, steps per second:  65, episode reward: 254.000, mean reward:  0.992 [ 0.000,  1.000], mean action: 4.961 [0.000, 11.000],  loss: 0.027766, mae: 0.894162, mean_q: 1.250536\n",
      "done, took 3.942 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.005s, episode steps: 256, steps per second:  64, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.938 [0.000, 11.000],  loss: 0.028462, mae: 0.892792, mean_q: 1.239001\n",
      "done, took 4.007 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.767s, episode steps: 256, steps per second:  68, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.949 [0.000, 11.000],  loss: 0.026160, mae: 0.901995, mean_q: 1.242309\n",
      "done, took 3.768 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.774s, episode steps: 256, steps per second:  68, episode reward: 254.000, mean reward:  0.992 [ 0.000,  1.000], mean action: 4.969 [0.000, 11.000],  loss: 0.023424, mae: 0.918630, mean_q: 1.251137\n",
      "done, took 3.775 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.711s, episode steps: 256, steps per second:  69, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.965 [0.000, 11.000],  loss: 0.024434, mae: 0.913726, mean_q: 1.235385\n",
      "done, took 3.712 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.793s, episode steps: 256, steps per second:  67, episode reward: 255.000, mean reward:  0.996 [ 0.000,  1.000], mean action: 4.965 [0.000, 11.000],  loss: 0.021501, mae: 0.888638, mean_q: 1.194497\n",
      "done, took 3.795 seconds\n",
      "  g_loss: 0.63130975\n",
      "Evasions: 1528\n",
      "Time left = 57.6 minutes\n",
      "Total Time Taken: 1.6 minutes\n",
      "epoch_number: 4 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.801s, episode steps: 256, steps per second:  67, episode reward: 250.000, mean reward:  0.977 [ 0.000,  1.000], mean action: 4.957 [0.000, 11.000],  loss: 0.022574, mae: 0.903031, mean_q: 1.200738\n",
      "done, took 3.803 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.688s, episode steps: 256, steps per second:  69, episode reward: 252.000, mean reward:  0.984 [ 0.000,  1.000], mean action: 4.973 [0.000, 11.000],  loss: 0.020041, mae: 0.917434, mean_q: 1.208143\n",
      "done, took 3.690 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.625s, episode steps: 256, steps per second:  71, episode reward: 252.000, mean reward:  0.984 [ 0.000,  1.000], mean action: 4.996 [0.000, 11.000],  loss: 0.020195, mae: 0.915658, mean_q: 1.195699\n",
      "done, took 3.627 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.864s, episode steps: 256, steps per second:  66, episode reward: 252.000, mean reward:  0.984 [ 0.000,  1.000], mean action: 4.988 [0.000, 11.000],  loss: 0.018610, mae: 0.918003, mean_q: 1.196684\n",
      "done, took 3.866 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.774s, episode steps: 256, steps per second:  68, episode reward: 252.000, mean reward:  0.984 [ 0.000,  1.000], mean action: 4.988 [0.000, 11.000],  loss: 0.017633, mae: 0.921692, mean_q: 1.190982\n",
      "done, took 3.776 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.659s, episode steps: 256, steps per second:  70, episode reward: 248.000, mean reward:  0.969 [ 0.000,  1.000], mean action: 4.953 [0.000, 11.000],  loss: 0.018269, mae: 0.921277, mean_q: 1.186825\n",
      "done, took 3.661 seconds\n",
      "  g_loss: 0.61144567\n",
      "Evasions: 1506\n",
      "Time left = 57.6 minutes\n",
      "Total Time Taken: 2.0 minutes\n",
      "epoch_number: 5 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.117s, episode steps: 256, steps per second:  62, episode reward: 129.000, mean reward:  0.504 [ 0.000,  1.000], mean action: 4.977 [0.000, 11.000],  loss: 0.022999, mae: 0.920041, mean_q: 1.174561\n",
      "done, took 4.119 seconds\n",
      "Training for 256 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256/256: episode: 1, duration: 4.001s, episode steps: 256, steps per second:  64, episode reward: 126.000, mean reward:  0.492 [ 0.000,  1.000], mean action: 4.961 [0.000, 11.000],  loss: 0.033025, mae: 0.895859, mean_q: 1.153208\n",
      "done, took 4.002 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.956s, episode steps: 256, steps per second:  65, episode reward: 131.000, mean reward:  0.512 [ 0.000,  1.000], mean action: 4.973 [0.000, 11.000],  loss: 0.040924, mae: 0.880809, mean_q: 1.129381\n",
      "done, took 3.958 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.100s, episode steps: 256, steps per second:  62, episode reward: 135.000, mean reward:  0.527 [ 0.000,  1.000], mean action: 4.992 [0.000, 11.000],  loss: 0.045606, mae: 0.875361, mean_q: 1.117549\n",
      "done, took 4.102 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.278s, episode steps: 256, steps per second:  60, episode reward: 139.000, mean reward:  0.543 [ 0.000,  1.000], mean action: 4.996 [0.000, 11.000],  loss: 0.050669, mae: 0.864024, mean_q: 1.096216\n",
      "done, took 4.280 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.117s, episode steps: 256, steps per second:  62, episode reward: 125.000, mean reward:  0.488 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.054539, mae: 0.854348, mean_q: 1.091252\n",
      "done, took 4.119 seconds\n",
      "  g_loss: 0.7026749\n",
      "Evasions: 785\n",
      "Time left = 58.2 minutes\n",
      "Total Time Taken: 2.4 minutes\n",
      "epoch_number: 6 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.410s, episode steps: 256, steps per second:  58, episode reward: 126.000, mean reward:  0.492 [ 0.000,  1.000], mean action: 5.059 [0.000, 11.000],  loss: 0.063095, mae: 0.835518, mean_q: 1.064953\n",
      "done, took 4.412 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.221s, episode steps: 256, steps per second:  61, episode reward: 142.000, mean reward:  0.555 [ 0.000,  1.000], mean action: 4.988 [0.000, 11.000],  loss: 0.065163, mae: 0.840946, mean_q: 1.070301\n",
      "done, took 4.223 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.347s, episode steps: 256, steps per second:  59, episode reward: 136.000, mean reward:  0.531 [ 0.000,  1.000], mean action: 5.012 [0.000, 11.000],  loss: 0.072222, mae: 0.833196, mean_q: 1.055659\n",
      "done, took 4.349 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.224s, episode steps: 256, steps per second:  61, episode reward: 124.000, mean reward:  0.484 [ 0.000,  1.000], mean action: 5.055 [0.000, 11.000],  loss: 0.072694, mae: 0.818931, mean_q: 1.043537\n",
      "done, took 4.225 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.202s, episode steps: 256, steps per second:  61, episode reward: 130.000, mean reward:  0.508 [ 0.000,  1.000], mean action: 5.082 [0.000, 11.000],  loss: 0.076547, mae: 0.809547, mean_q: 1.025456\n",
      "done, took 4.204 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.096s, episode steps: 256, steps per second:  63, episode reward: 129.000, mean reward:  0.504 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.080768, mae: 0.813036, mean_q: 1.032121\n",
      "done, took 4.097 seconds\n",
      "  g_loss: 0.58413255\n",
      "Evasions: 787\n",
      "Time left = 58.8 minutes\n",
      "Total Time Taken: 2.9 minutes\n",
      "epoch_number: 7 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.390s, episode steps: 256, steps per second:  58, episode reward: 124.000, mean reward:  0.484 [ 0.000,  1.000], mean action: 5.066 [0.000, 11.000],  loss: 0.089553, mae: 0.797930, mean_q: 1.010203\n",
      "done, took 4.391 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.202s, episode steps: 256, steps per second:  61, episode reward: 139.000, mean reward:  0.543 [ 0.000,  1.000], mean action: 5.035 [0.000, 11.000],  loss: 0.082096, mae: 0.803850, mean_q: 1.017038\n",
      "done, took 4.204 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.195s, episode steps: 256, steps per second:  61, episode reward: 128.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.086545, mae: 0.793161, mean_q: 1.001327\n",
      "done, took 4.197 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.188s, episode steps: 256, steps per second:  61, episode reward: 112.000, mean reward:  0.438 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.093316, mae: 0.785093, mean_q: 0.986869\n",
      "done, took 4.189 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.134s, episode steps: 256, steps per second:  62, episode reward: 132.000, mean reward:  0.516 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.095251, mae: 0.785433, mean_q: 0.990573\n",
      "done, took 4.135 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.218s, episode steps: 256, steps per second:  61, episode reward: 113.000, mean reward:  0.441 [ 0.000,  1.000], mean action: 5.023 [0.000, 11.000],  loss: 0.097285, mae: 0.791161, mean_q: 0.995068\n",
      "done, took 4.220 seconds\n",
      "  g_loss: 0.45431\n",
      "Evasions: 748\n",
      "Time left = 59.4 minutes\n",
      "Total Time Taken: 3.3 minutes\n",
      "epoch_number: 8 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.184s, episode steps: 256, steps per second:  61, episode reward: 126.000, mean reward:  0.492 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.100290, mae: 0.778069, mean_q: 0.975435\n",
      "done, took 4.186 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.128s, episode steps: 256, steps per second:  62, episode reward: 131.000, mean reward:  0.512 [ 0.000,  1.000], mean action: 5.012 [0.000, 11.000],  loss: 0.100263, mae: 0.768461, mean_q: 0.967336\n",
      "done, took 4.130 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.084s, episode steps: 256, steps per second:  63, episode reward: 116.000, mean reward:  0.453 [ 0.000,  1.000], mean action: 5.016 [0.000, 11.000],  loss: 0.101035, mae: 0.759572, mean_q: 0.956131\n",
      "done, took 4.086 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.759s, episode steps: 256, steps per second:  68, episode reward: 91.000, mean reward:  0.355 [ 0.000,  1.000], mean action: 5.020 [0.000, 11.000],  loss: 0.108960, mae: 0.761044, mean_q: 0.955877\n",
      "done, took 3.760 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.021s, episode steps: 256, steps per second:  64, episode reward: 108.000, mean reward:  0.422 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.105558, mae: 0.758393, mean_q: 0.947140\n",
      "done, took 4.023 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.804s, episode steps: 256, steps per second:  67, episode reward: 46.000, mean reward:  0.180 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.113471, mae: 0.748341, mean_q: 0.935412\n",
      "done, took 3.806 seconds\n",
      "  g_loss: 0.5080406\n",
      "Evasions: 618\n",
      "Time left = 58.8 minutes\n",
      "Total Time Taken: 3.8 minutes\n",
      "epoch_number: 9 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.926s, episode steps: 256, steps per second:  65, episode reward: 111.000, mean reward:  0.434 [ 0.000,  1.000], mean action: 5.016 [0.000, 11.000],  loss: 0.110542, mae: 0.742380, mean_q: 0.933198\n",
      "done, took 3.928 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.792s, episode steps: 256, steps per second:  68, episode reward: 40.000, mean reward:  0.156 [ 0.000,  1.000], mean action: 5.004 [0.000, 11.000],  loss: 0.113991, mae: 0.739789, mean_q: 0.922166\n",
      "done, took 3.794 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.729s, episode steps: 256, steps per second:  69, episode reward: 17.000, mean reward:  0.066 [ 0.000,  1.000], mean action: 5.016 [0.000, 11.000],  loss: 0.117740, mae: 0.724849, mean_q: 0.914075\n",
      "done, took 3.730 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.781s, episode steps: 256, steps per second:  68, episode reward: 35.000, mean reward:  0.137 [ 0.000,  1.000], mean action: 5.074 [0.000, 11.000],  loss: 0.115705, mae: 0.700374, mean_q: 0.884496\n",
      "done, took 3.782 seconds\n",
      "Training for 256 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256/256: episode: 1, duration: 3.712s, episode steps: 256, steps per second:  69, episode reward: 36.000, mean reward:  0.141 [ 0.000,  1.000], mean action: 5.020 [0.000, 11.000],  loss: 0.119941, mae: 0.708754, mean_q: 0.884821\n",
      "done, took 3.714 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.964s, episode steps: 256, steps per second:  65, episode reward: 28.000, mean reward:  0.109 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.116093, mae: 0.694025, mean_q: 0.872885\n",
      "done, took 3.966 seconds\n",
      "  g_loss: 0.44012237\n",
      "Evasions: 267\n",
      "Time left = 58.2 minutes\n",
      "Total Time Taken: 4.2 minutes\n",
      "epoch_number: 10 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.779s, episode steps: 256, steps per second:  68, episode reward: 116.000, mean reward:  0.453 [ 0.000,  1.000], mean action: 5.016 [0.000, 11.000],  loss: 0.126996, mae: 0.701836, mean_q: 0.874556\n",
      "done, took 3.781 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.688s, episode steps: 256, steps per second:  69, episode reward: 22.000, mean reward:  0.086 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.128447, mae: 0.700029, mean_q: 0.877981\n",
      "done, took 3.690 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.879s, episode steps: 256, steps per second:  66, episode reward: 18.000, mean reward:  0.070 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.126857, mae: 0.677182, mean_q: 0.855956\n",
      "done, took 3.880 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.655s, episode steps: 256, steps per second:  70, episode reward:  9.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 5.078 [0.000, 11.000],  loss: 0.124288, mae: 0.690834, mean_q: 0.872692\n",
      "done, took 3.657 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.885s, episode steps: 256, steps per second:  66, episode reward: 16.000, mean reward:  0.062 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.131975, mae: 0.668515, mean_q: 0.838336\n",
      "done, took 3.886 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.845s, episode steps: 256, steps per second:  67, episode reward:  6.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 5.051 [0.000, 11.000],  loss: 0.129600, mae: 0.649003, mean_q: 0.814845\n",
      "done, took 3.846 seconds\n",
      "  g_loss: 0.4966662\n",
      "Evasions: 187\n",
      "Time left = 57.6 minutes\n",
      "Total Time Taken: 4.6 minutes\n",
      "epoch_number: 11 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.892s, episode steps: 256, steps per second:  66, episode reward: 13.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 5.074 [0.000, 11.000],  loss: 0.131296, mae: 0.644598, mean_q: 0.813402\n",
      "done, took 3.894 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.824s, episode steps: 256, steps per second:  67, episode reward: 16.000, mean reward:  0.062 [ 0.000,  1.000], mean action: 5.066 [0.000, 11.000],  loss: 0.131385, mae: 0.648629, mean_q: 0.815887\n",
      "done, took 3.826 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.797s, episode steps: 256, steps per second:  67, episode reward: 13.000, mean reward:  0.051 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.133150, mae: 0.640219, mean_q: 0.805979\n",
      "done, took 3.799 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.885s, episode steps: 256, steps per second:  66, episode reward: 11.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 5.055 [0.000, 11.000],  loss: 0.133677, mae: 0.617560, mean_q: 0.777366\n",
      "done, took 3.886 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.838s, episode steps: 256, steps per second:  67, episode reward: 17.000, mean reward:  0.066 [ 0.000,  1.000], mean action: 5.062 [0.000, 11.000],  loss: 0.138728, mae: 0.613265, mean_q: 0.784659\n",
      "done, took 3.840 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.748s, episode steps: 256, steps per second:  68, episode reward: 19.000, mean reward:  0.074 [ 0.000,  1.000], mean action: 5.055 [0.000, 11.000],  loss: 0.134311, mae: 0.622472, mean_q: 0.788685\n",
      "done, took 3.750 seconds\n",
      "  g_loss: 0.4078998\n",
      "Evasions: 89\n",
      "Time left = 57.6 minutes\n",
      "Total Time Taken: 5.0 minutes\n",
      "epoch_number: 12 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.728s, episode steps: 256, steps per second:  69, episode reward: 12.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 5.055 [0.000, 11.000],  loss: 0.137436, mae: 0.618834, mean_q: 0.780607\n",
      "done, took 3.729 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.792s, episode steps: 256, steps per second:  68, episode reward: 10.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 5.059 [0.000, 11.000],  loss: 0.138912, mae: 0.598738, mean_q: 0.754563\n",
      "done, took 3.794 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.744s, episode steps: 256, steps per second:  68, episode reward: 17.000, mean reward:  0.066 [ 0.000,  1.000], mean action: 5.023 [0.000, 11.000],  loss: 0.138092, mae: 0.598819, mean_q: 0.753520\n",
      "done, took 3.745 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.832s, episode steps: 256, steps per second:  67, episode reward: 14.000, mean reward:  0.055 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.139650, mae: 0.591251, mean_q: 0.745950\n",
      "done, took 3.834 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.812s, episode steps: 256, steps per second:  67, episode reward:  8.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 5.027 [0.000, 11.000],  loss: 0.137819, mae: 0.598063, mean_q: 0.749763\n",
      "done, took 3.814 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.774s, episode steps: 256, steps per second:  68, episode reward: 14.000, mean reward:  0.055 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.138663, mae: 0.588590, mean_q: 0.739406\n",
      "done, took 3.777 seconds\n",
      "  g_loss: 0.43558025\n",
      "Evasions: 75\n",
      "Time left = 57.0 minutes\n",
      "Total Time Taken: 5.4 minutes\n",
      "epoch_number: 13 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.804s, episode steps: 256, steps per second:  67, episode reward: 59.000, mean reward:  0.230 [ 0.000,  1.000], mean action: 5.059 [0.000, 11.000],  loss: 0.140623, mae: 0.581300, mean_q: 0.735916\n",
      "done, took 3.807 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.859s, episode steps: 256, steps per second:  66, episode reward: 18.000, mean reward:  0.070 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.139209, mae: 0.569469, mean_q: 0.725044\n",
      "done, took 3.861 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.088s, episode steps: 256, steps per second:  63, episode reward: 10.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 5.070 [0.000, 11.000],  loss: 0.140532, mae: 0.585388, mean_q: 0.728172\n",
      "done, took 4.090 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.058s, episode steps: 256, steps per second:  63, episode reward: 17.000, mean reward:  0.066 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.138206, mae: 0.564028, mean_q: 0.708487\n",
      "done, took 4.059 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.065s, episode steps: 256, steps per second:  63, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.059 [0.000, 11.000],  loss: 0.137686, mae: 0.567218, mean_q: 0.716811\n",
      "done, took 4.066 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.966s, episode steps: 256, steps per second:  65, episode reward: 11.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.137549, mae: 0.554952, mean_q: 0.701222\n",
      "done, took 3.967 seconds\n",
      "  g_loss: 0.4069978\n",
      "Evasions: 117\n",
      "Time left = 56.4 minutes\n",
      "Total Time Taken: 5.8 minutes\n",
      "epoch_number: 14 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.958s, episode steps: 256, steps per second:  65, episode reward: 12.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.141479, mae: 0.572341, mean_q: 0.715555\n",
      "done, took 3.960 seconds\n",
      "Training for 256 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256/256: episode: 1, duration: 3.823s, episode steps: 256, steps per second:  67, episode reward: 21.000, mean reward:  0.082 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.136597, mae: 0.548596, mean_q: 0.699333\n",
      "done, took 3.825 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.779s, episode steps: 256, steps per second:  68, episode reward:  8.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 5.078 [0.000, 11.000],  loss: 0.134072, mae: 0.532138, mean_q: 0.687950\n",
      "done, took 3.781 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.838s, episode steps: 256, steps per second:  67, episode reward:  7.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 5.066 [0.000, 11.000],  loss: 0.133037, mae: 0.519580, mean_q: 0.668975\n",
      "done, took 3.840 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.798s, episode steps: 256, steps per second:  67, episode reward:  7.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 5.027 [0.000, 11.000],  loss: 0.134015, mae: 0.523386, mean_q: 0.664238\n",
      "done, took 3.800 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.783s, episode steps: 256, steps per second:  68, episode reward:  6.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 5.051 [0.000, 11.000],  loss: 0.140274, mae: 0.526912, mean_q: 0.669052\n",
      "done, took 3.785 seconds\n",
      "  g_loss: 0.3853658\n",
      "Evasions: 61\n",
      "Time left = 56.4 minutes\n",
      "Total Time Taken: 6.2 minutes\n",
      "epoch_number: 15 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.864s, episode steps: 256, steps per second:  66, episode reward: 11.000, mean reward:  0.043 [ 0.000,  1.000], mean action: 5.020 [0.000, 11.000],  loss: 0.134169, mae: 0.524396, mean_q: 0.666037\n",
      "done, took 3.866 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 4.137s, episode steps: 256, steps per second:  62, episode reward:  7.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.135572, mae: 0.523075, mean_q: 0.664420\n",
      "done, took 4.139 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.897s, episode steps: 256, steps per second:  66, episode reward: 12.000, mean reward:  0.047 [ 0.000,  1.000], mean action: 5.035 [0.000, 11.000],  loss: 0.135702, mae: 0.520524, mean_q: 0.663811\n",
      "done, took 3.899 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.880s, episode steps: 256, steps per second:  66, episode reward:  6.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 5.016 [0.000, 11.000],  loss: 0.133162, mae: 0.506081, mean_q: 0.651863\n",
      "done, took 3.882 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.895s, episode steps: 256, steps per second:  66, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.051 [0.000, 11.000],  loss: 0.134621, mae: 0.497007, mean_q: 0.632354\n",
      "done, took 3.897 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.914s, episode steps: 256, steps per second:  65, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.134249, mae: 0.508460, mean_q: 0.639800\n",
      "done, took 3.916 seconds\n",
      "  g_loss: 0.3570968\n",
      "Evasions: 39\n",
      "Time left = 55.8 minutes\n",
      "Total Time Taken: 6.7 minutes\n",
      "epoch_number: 16 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.834s, episode steps: 256, steps per second:  67, episode reward: 10.000, mean reward:  0.039 [ 0.000,  1.000], mean action: 5.051 [0.000, 11.000],  loss: 0.130220, mae: 0.490284, mean_q: 0.624162\n",
      "done, took 3.836 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.860s, episode steps: 256, steps per second:  66, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.132224, mae: 0.480842, mean_q: 0.619589\n",
      "done, took 3.862 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.810s, episode steps: 256, steps per second:  67, episode reward:  8.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 5.062 [0.000, 11.000],  loss: 0.130143, mae: 0.492301, mean_q: 0.627619\n",
      "done, took 3.812 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.795s, episode steps: 256, steps per second:  67, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.133590, mae: 0.485196, mean_q: 0.622779\n",
      "done, took 3.797 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.936s, episode steps: 256, steps per second:  65, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 4.996 [0.000, 11.000],  loss: 0.129039, mae: 0.485219, mean_q: 0.613794\n",
      "done, took 3.938 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.661s, episode steps: 256, steps per second:  70, episode reward:  8.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 5.012 [0.000, 11.000],  loss: 0.128877, mae: 0.485604, mean_q: 0.623501\n",
      "done, took 3.662 seconds\n",
      "  g_loss: 0.36358136\n",
      "Evasions: 35\n",
      "Time left = 55.2 minutes\n",
      "Total Time Taken: 7.1 minutes\n",
      "epoch_number: 17 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.805s, episode steps: 256, steps per second:  67, episode reward:  9.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 5.023 [0.000, 11.000],  loss: 0.128715, mae: 0.475634, mean_q: 0.601574\n",
      "done, took 3.807 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.814s, episode steps: 256, steps per second:  67, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 4.988 [0.000, 11.000],  loss: 0.128744, mae: 0.482506, mean_q: 0.608064\n",
      "done, took 3.816 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.837s, episode steps: 256, steps per second:  67, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.008 [0.000, 11.000],  loss: 0.128511, mae: 0.473004, mean_q: 0.602196\n",
      "done, took 3.838 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.971s, episode steps: 256, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.051 [0.000, 11.000],  loss: 0.128050, mae: 0.475594, mean_q: 0.605395\n",
      "done, took 3.972 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.666s, episode steps: 256, steps per second:  70, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 4.977 [0.000, 11.000],  loss: 0.125024, mae: 0.461972, mean_q: 0.587237\n",
      "done, took 3.667 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.808s, episode steps: 256, steps per second:  67, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 4.996 [0.000, 11.000],  loss: 0.130321, mae: 0.466710, mean_q: 0.587748\n",
      "done, took 3.810 seconds\n",
      "  g_loss: 0.29354376\n",
      "Evasions: 17\n",
      "Time left = 54.6 minutes\n",
      "Total Time Taken: 7.5 minutes\n",
      "epoch_number: 18 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.897s, episode steps: 256, steps per second:  66, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 4.992 [0.000, 11.000],  loss: 0.124565, mae: 0.467353, mean_q: 0.594768\n",
      "done, took 3.899 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.855s, episode steps: 256, steps per second:  66, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 4.992 [0.000, 11.000],  loss: 0.126855, mae: 0.456919, mean_q: 0.588308\n",
      "done, took 3.856 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.889s, episode steps: 256, steps per second:  66, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.020 [0.000, 11.000],  loss: 0.128759, mae: 0.440675, mean_q: 0.570891\n",
      "done, took 3.892 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.814s, episode steps: 256, steps per second:  67, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.125299, mae: 0.460614, mean_q: 0.588753\n",
      "done, took 3.816 seconds\n",
      "Training for 256 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 256/256: episode: 1, duration: 3.721s, episode steps: 256, steps per second:  69, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.125247, mae: 0.452138, mean_q: 0.576060\n",
      "done, took 3.723 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.914s, episode steps: 256, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.043 [0.000, 11.000],  loss: 0.122427, mae: 0.455138, mean_q: 0.588338\n",
      "done, took 3.916 seconds\n",
      "  g_loss: 0.31557196\n",
      "Evasions: 11\n",
      "Time left = 54.6 minutes\n",
      "Total Time Taken: 7.9 minutes\n",
      "epoch_number: 19 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.899s, episode steps: 256, steps per second:  66, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.047 [0.000, 11.000],  loss: 0.123748, mae: 0.445601, mean_q: 0.561150\n",
      "done, took 3.901 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.788s, episode steps: 256, steps per second:  68, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 5.051 [0.000, 11.000],  loss: 0.121687, mae: 0.443638, mean_q: 0.565261\n",
      "done, took 3.789 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.807s, episode steps: 256, steps per second:  67, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.000 [0.000, 11.000],  loss: 0.118992, mae: 0.437307, mean_q: 0.558210\n",
      "done, took 3.809 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.964s, episode steps: 256, steps per second:  65, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.119295, mae: 0.435952, mean_q: 0.566813\n",
      "done, took 3.965 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.709s, episode steps: 256, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.023 [0.000, 11.000],  loss: 0.123062, mae: 0.431296, mean_q: 0.554341\n",
      "done, took 3.711 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.710s, episode steps: 256, steps per second:  69, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.051 [0.000, 11.000],  loss: 0.122065, mae: 0.426055, mean_q: 0.546478\n",
      "done, took 3.712 seconds\n",
      "  g_loss: 0.3041473\n",
      "Evasions: 9\n",
      "Time left = 54.0 minutes\n",
      "Total Time Taken: 8.3 minutes\n",
      "epoch_number: 20 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.836s, episode steps: 256, steps per second:  67, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 5.043 [0.000, 11.000],  loss: 0.118260, mae: 0.434037, mean_q: 0.558101\n",
      "done, took 3.838 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.758s, episode steps: 256, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.035 [0.000, 11.000],  loss: 0.115571, mae: 0.433647, mean_q: 0.556310\n",
      "done, took 3.760 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.792s, episode steps: 256, steps per second:  68, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.066 [0.000, 11.000],  loss: 0.120130, mae: 0.430300, mean_q: 0.548605\n",
      "done, took 3.794 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.818s, episode steps: 256, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.039 [0.000, 11.000],  loss: 0.117709, mae: 0.439765, mean_q: 0.553628\n",
      "done, took 3.820 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.738s, episode steps: 256, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.023 [0.000, 11.000],  loss: 0.120137, mae: 0.447013, mean_q: 0.563549\n",
      "done, took 3.739 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.883s, episode steps: 256, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.055 [0.000, 11.000],  loss: 0.118383, mae: 0.430672, mean_q: 0.549014\n",
      "done, took 3.884 seconds\n",
      "  g_loss: 0.3013928\n",
      "Evasions: 6\n",
      "Time left = 53.4 minutes\n",
      "Total Time Taken: 8.7 minutes\n",
      "epoch_number: 21 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.929s, episode steps: 256, steps per second:  65, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 5.031 [0.000, 11.000],  loss: 0.116036, mae: 0.427055, mean_q: 0.545648\n",
      "done, took 3.930 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.970s, episode steps: 256, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 5.039 [0.000, 11.000],  loss: 0.118556, mae: 0.396582, mean_q: 0.516164\n",
      "done, took 3.972 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.744s, episode steps: 256, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.023 [0.000, 11.000],  loss: 0.113854, mae: 0.422320, mean_q: 0.537972\n",
      "done, took 3.745 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.956s, episode steps: 256, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.027 [0.000, 11.000],  loss: 0.116345, mae: 0.409347, mean_q: 0.517951\n",
      "done, took 3.958 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.986s, episode steps: 256, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 5.055 [0.000, 11.000],  loss: 0.115013, mae: 0.420859, mean_q: 0.530588\n",
      "done, took 3.987 seconds\n",
      "Training for 256 steps ...\n",
      " 256/256: episode: 1, duration: 3.865s, episode steps: 256, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5.031 [0.000, 11.000],  loss: 0.112624, mae: 0.424439, mean_q: 0.543145\n",
      "done, took 3.867 seconds\n",
      "  g_loss: 0.28577575\n",
      "Evasions: 4\n",
      "Time left = 52.8 minutes\n",
      "Total Time Taken: 9.1 minutes\n",
      "epoch_number: 22 completed\n",
      "======================================================\n",
      "Training for 256 steps ...\n"
     ]
    }
   ],
   "source": [
    "import header\n",
    "import importlib\n",
    "importlib.reload(header) # For reloading after making changes\n",
    "from header import *\n",
    "\n",
    "\n",
    "gpu_device = '/device:GPU:1'\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for gpu_instance in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)\n",
    "\n",
    "#----------------------------------\n",
    "# Set neurons and batch size\n",
    "#----------------------------------\n",
    "base_n_count = 256\n",
    "batch_size =  256\n",
    "#----------------------------------\n",
    "\n",
    "\n",
    "result = train_data\n",
    "\n",
    "remaining = train_data.shape[0] % batch_size\n",
    "\n",
    "if remaining > 0:\n",
    "    if remaining < train_data.shape[0]:\n",
    "        additional = batch_size - remaining\n",
    "        _additional = train_data.loc[train_data.shape[0]-additional:train_data.shape[0],: ]  \n",
    "        \n",
    "        frames = [train_data, _additional]\n",
    "        result = pd.concat(frames).reset_index(drop=True)\n",
    "\n",
    "print('Result: ' + str(result.shape))\n",
    "\n",
    "# batch_size = 1\n",
    "# ---------------------------------\n",
    "nb_steps = required_epochs * result.shape[0] // batch_size\n",
    "\n",
    "log_interval = result.shape[0] // batch_size # We are setting this as an epoch. This depends on data size.\n",
    "\n",
    "print(\"log_interval : \" + str(log_interval))\n",
    "\n",
    "# nb_steps = TRAINING_ITERATIONS  # 50000 # Add one for logging of the last interval\n",
    "print(\"Total Batch Iterations: \" + str(nb_steps))\n",
    "rand_noise_dim = 100 \n",
    "\n",
    "\n",
    "k_d = 1  # number of critic network updates per adversarial training step\n",
    "k_g = 1  # number of generator network updates per adversarial training step\n",
    "\n",
    "critic_pre_train_steps = 100# 100  # number of steps to pre-train the critic before starting adversarial training\n",
    "\n",
    "generator_model_path, discriminator_model_path, loss_pickle_path = None, None, None\n",
    "\n",
    "show = True \n",
    "train = result#.copy().reset_index(drop=True) # botnet only with labels from classification\n",
    "\n",
    "\n",
    "\n",
    "label_cols = [ i for i in train.columns if 'Label' in i ]\n",
    "\n",
    "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
    "\n",
    "print(data_cols)\n",
    "\n",
    "train_no_label = train[ data_cols ]\n",
    "\n",
    "train_no_label = round(train_no_label, 4)\n",
    "\n",
    "# if SAVE_ONLY_BOT_DATA:\n",
    "#     train_no_label.to_csv(str(DATA_SET_PATH) + 'ONLY_BOTNET_DATA_(Preprocessed).csv')\n",
    "#     print('File: ' + 'ONLY_BOTNET_DATA_(Preprocessed).csv saved to directory')   \n",
    "\n",
    "\n",
    "\n",
    "test_size = train.shape[0] \n",
    "learning_rate = 5e-4\n",
    "\n",
    "\n",
    "TODAY = DATA_SET + '_' + str(datetime.datetime.now()) \n",
    "\n",
    "print(TODAY)\n",
    "\n",
    "\n",
    "arguments = [rand_noise_dim, nb_steps, batch_size, \n",
    "            k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "            CACHE_PATH, FIGS_PATH, show, test_size, gpu_device, EVALUATION_PARAMETER, TODAY, DATA_SET]\n",
    "\n",
    "best_losses = train_RELEVAGAN_CC(arguments, train, Train, data_cols)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "RELEVAGAN.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0754585e5bea998e5d67e8f88be1e2a4051f453a7d5aedf516d053743049d686"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
